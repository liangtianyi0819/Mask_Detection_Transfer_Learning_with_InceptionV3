{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB-kW9JnHYdK"
   },
   "source": [
    "# Mask Detection using Transfer Learning based on InceptionV3 Neural Network\n",
    "Author: Tianyi Liang\n",
    "\n",
    "This project is divided into several sections:\n",
    "\n",
    "1. [Import Necessary Packages](#import-packages)\n",
    "2. [Download the Dataset](#download-dataset)\n",
    "3. [Read the Data into Memory](#read-data)\n",
    "4. [Perform Data Visualization](#data-visualization)\n",
    "5. [Normalize the Data and Format it into the Required Shape](#normalize-data)\n",
    "6. [Check the Balance of the Data](#check-balance)\n",
    "7. [Implement Image Augmentation to Prevent Overfitting](#image-augmentation)\n",
    "8. [Build the Network using InceptionV3 and a Custom Classifier](#build-network)\n",
    "9. [Train the Model on the Augmented Data](#train-model)\n",
    "10. [Evaluate the Model and Analyze Wrong Cases](#evaluate-model)\n",
    "\n",
    "## Requirements:\n",
    "- Python 3.7\n",
    "- TensorFlow\n",
    "- Keras\n",
    "- OpenCV\n",
    "- opendatasets\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Seaborn\n",
    "- Matplotlib\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8e0yTJktvxY",
    "outputId": "033dd8b0-cc08-4eb3-f6e0-de01b3cc7064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: opendatasets in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.22)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opendatasets) (4.64.1)\n",
      "Requirement already satisfied: kaggle in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opendatasets) (1.5.12)\n",
      "Requirement already satisfied: click in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opendatasets) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->opendatasets) (0.4.4)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle->opendatasets) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle->opendatasets) (2022.9.24)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle->opendatasets) (2.28.1)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle->opendatasets) (7.0.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kaggle->opendatasets) (1.26.13)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->kaggle->opendatasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->kaggle->opendatasets) (3.4)\n",
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -upyterlab (c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -upyterlab (c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: opencv-python in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opencv-python) (1.22.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -upyterlab (c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -upyterlab (c:\\users\\harry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Bindings generation error. Submodule name should always start with a parent module name. Parent name: cv2.cv2. Submodule name: cv2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 15>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mopendatasets\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mod\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cv2\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pyplot \u001B[38;5;28;01mas\u001B[39;00m plt\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n",
      "\u001B[1;31mImportError\u001B[0m: Bindings generation error. Submodule name should always start with a parent module name. Parent name: cv2.cv2. Submodule name: cv2"
     ]
    }
   ],
   "source": [
    "##Import necessary packages\n",
    "!pip install opendatasets\n",
    "!pip install opencv-python\n",
    "!pip install opencv-contrib-python\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import opendatasets as od\n",
    "\n",
    "from cv2 import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation, Input, Add, \\\n",
    "                   BatchNormalization, Flatten, Conv2D, \\\n",
    "                   AveragePooling2D, MaxPooling2D, ZeroPadding2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTh3lMfEH6vu"
   },
   "source": [
    "## Download the dataset in public domain of Kaggle\n",
    "#### ( https://www.kaggle.com/datasets/ashishjangra27/face-mask-12k-images-dataset uploaded by ASHISH JANGRA)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BEQhzQLtvtQ",
    "outputId": "131fd8d4-e600-4ef8-c8c3-d2d7bf06dc33"
   },
   "outputs": [],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/ashishjangra27/face-mask-12k-images-dataset\")\n",
    "root_path = \"face-mask-12k-images-dataset/Face Mask Dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXd3wTngII3l"
   },
   "source": [
    "## Read the data into memory\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fd4cN8t5vTgW",
    "outputId": "6ace4040-d47f-4bff-fc7b-e1b92f335d01"
   },
   "outputs": [],
   "source": [
    "dataset = {\"path\": [], \"classification\": [], \"set\": [], \"value\": []}\n",
    "\n",
    "# Train or test folder\n",
    "for train_test in os.listdir(root_path):\n",
    "\n",
    "    # Mask or WithoutMask folder\n",
    "    for true_false in os.listdir(root_path+\"/\"+train_test):\n",
    "\n",
    "        # Each image inside\n",
    "        for image in glob.glob(root_path + \n",
    "                     train_test + \"/\" + \n",
    "                     true_false + \"/\" + \n",
    "                     \"*.png\"):\n",
    "\n",
    "            this_path = str(root_path + \n",
    "                     train_test + \"/\" + \n",
    "                     true_false + \"/\" + \n",
    "                     image)\n",
    "            img = cv2.imread(image, 3)\n",
    "            img = np.asarray(cv2.resize(img, \n",
    "                    dsize=(224, 224)))\n",
    "            img = cv2.merge(cv2.split(img)[::-1])\n",
    "            dataset[\"path\"].append(image)\n",
    "            if true_false == \"WithMask\":\n",
    "                dataset[\"classification\"].append(np.uint8(1))\n",
    "            else:\n",
    "                dataset[\"classification\"].append(np.uint8(0))\n",
    "            dataset[\"set\"].append(train_test.lower())\n",
    "            dataset[\"value\"].append(img)\n",
    "\n",
    "df_dataset = pd.DataFrame(dataset)\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_XICQkmISpz"
   },
   "source": [
    "## Take a look at the data \n",
    "### All images will be upsample/downsample to dimension **224 X 224 X 3** at this point.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Necessary Packages <a name=\"import-packages\"></a>\n",
    "In this stage, we import all the necessary packages required for the project. These include TensorFlow, Keras, OpenCV, opendatasets, Numpy, Pandas, Seaborn, and Matplotlib."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "id": "E8AKiAzowMSR",
    "outputId": "3e2bf262-5002-4289-9e9b-07163b133ae4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 15))\n",
    "np.random.seed(1)\n",
    "sample = np.random.choice(range(len(df_dataset)), size=25, replace=False)\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    img = df_dataset.loc[sample[i],\"value\"]\n",
    "    plt.imshow(img)\n",
    "    classification = 'WithMask' if df_dataset.loc[sample[i],\"classification\"] == 1 else \"WithoutMask\"\n",
    "    plt.title(classification, size = 15)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWrdB79eIjdK"
   },
   "source": [
    "## Perform max normalization on the data and format the data into the shape I need.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IsPapFBq0NLV",
    "outputId": "37451fe2-0054-4fe2-c655-3b40c3bb0da3"
   },
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    InitSeed = 23\n",
    "    tf.random.set_seed(InitSeed)\n",
    "    np.random.seed(InitSeed)\n",
    "\n",
    "\n",
    "train_set = pd.DataFrame(df_dataset[df_dataset[\"set\"] == \"train\"]).loc[:, [\"value\", \"classification\"]]\n",
    "test_set = pd.DataFrame(df_dataset[df_dataset[\"set\"] == \"test\"]).loc[:, [\"value\", \"classification\"]]\n",
    "valid_set = pd.DataFrame(df_dataset[df_dataset[\"set\"] == \"validation\"]).loc[:, [\"value\", \"classification\"]]\n",
    "\n",
    "x_train = np.asarray(train_set[\"value\"].tolist()) / 255\n",
    "y_train = np.asarray(train_set[\"classification\"].tolist())\n",
    "x_test = np.asarray(test_set[\"value\"].tolist()) / 255\n",
    "y_test = np.asarray(test_set[\"classification\"].tolist())\n",
    "x_valid = np.asarray(valid_set[\"value\"].tolist()) / 255\n",
    "y_valid = np.asarray(valid_set[\"classification\"].tolist())\n",
    "\n",
    "del df_dataset, train_set, test_set, valid_set, img, dataset, this_path\n",
    "del glob, od, pd, cv2\n",
    "gc.collect()\n",
    "print(\"x_train dimension:\", x_train.shape, \"\\ny_train dimension:\", y_train.shape,\n",
    "   \"\\nx_test dimension:\", x_test.shape, \"\\ny_test dimension:\", y_test.shape,\n",
    "   \"\\nx_valid dimension:\", x_valid.shape, \"\\ny_valid dimension:\", y_valid.shape,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW6xy_5wJYbJ"
   },
   "source": [
    "## Take a look at the balance of the data\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "-XyMKUCwDsop",
    "outputId": "a1253fae-9300-4fed-f9e9-12c71de54f91"
   },
   "outputs": [],
   "source": [
    "classes_distribution = {\"WithoutMask in Train\": np.sum(y_train == 0), \n",
    "              \"With in Train\": np.sum(y_train == 1),\n",
    "              \"WithoutMask in Test\": np.sum(y_test == 0),\n",
    "              \"With in Test\": np.sum(y_test == 1),\n",
    "              \"WithoutMask in Validation\": np.sum(y_valid == 0),\n",
    "              \"With in Validation\": np.sum(y_valid == 1)}\n",
    "plt.figure(figsize = (16, 9))\n",
    "color_A = (0.1, 0.4, 0.7)\n",
    "color_B = color_A[::-1]\n",
    "plt.bar(range(len(classes_distribution)), \n",
    "        list(classes_distribution.values()), \n",
    "        tick_label=list(classes_distribution.keys()),\n",
    "        color=[color_A, color_B])\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVs_k8YCJoZ2"
   },
   "source": [
    "## Implement image augmentation to put off overfitting\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "sDZqB-RpkKQc",
    "outputId": "804f1a62-5559-49e4-bb90-819209782637"
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "train_aug = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    height_shift_range=.1,\n",
    "    width_shift_range=.1,\n",
    "    zoom_range=.1,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "augment = train_aug.flow(x_train[0:1], batch_size=1)\n",
    "\n",
    "# Check if the augmentation works\n",
    "plt.figure(figsize = (20, 15))\n",
    "for i in range(1, 26):\n",
    "    plt.subplot(1 + 5 % i, 5, i)\n",
    "    tf.random.set_seed(1)\n",
    "    augment.reset()\n",
    "    plt.imshow(augment.next().squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "del augment\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNx-l0f-J28i"
   },
   "source": [
    "## Start building the network\n",
    "\n",
    "\n",
    "*   Implement InceptionV3 from Keras Application\n",
    "*   Implement self-designed W&DNN on the output of InceptionV3\n",
    "> *    Dense Layers\n",
    "> *    Choose of Kernel Initializers\n",
    "> *    Batch Normalization Layers\n",
    "> *    Choose of Activation Func\n",
    "> *    Dropout Layers\n",
    "*   Choose optimized compile options\n",
    "*   Implement self-designed learning rate scheduler\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gayA6Sqxrm0S"
   },
   "outputs": [],
   "source": [
    "def classifier_(input_):\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed=767)\n",
    "    output_ = keras.layers.GlobalAveragePooling2D()(input_)\n",
    "    output_ = keras.layers.Flatten()(output_)\n",
    "    output_ = keras.layers.BatchNormalization()(output_)\n",
    "    output_prime = output_\n",
    "\n",
    "    output_ = keras.layers.Dense(2048, kernel_initializer=initializer)(output_)\n",
    "    output_ = keras.layers.BatchNormalization()(output_)\n",
    "    output_ = keras.layers.Activation('relu')(output_)\n",
    "    output_ = keras.layers.Dropout(0.55)(output_)\n",
    "\n",
    "    output_ = keras.layers.Dense(2048, kernel_initializer=initializer)(output_)\n",
    "    output_ = keras.layers.BatchNormalization()(output_)\n",
    "    output_ = keras.layers.Activation('relu')(output_)\n",
    "    output_ = keras.layers.Dropout(0.55)(output_)\n",
    "\n",
    "    output_ = keras.layers.Dense(1024, kernel_initializer=initializer)(output_)\n",
    "    output_ = keras.layers.BatchNormalization()(output_)\n",
    "    output_ = keras.layers.Activation('relu')(output_)\n",
    "    output_ = keras.layers.Dropout(0.55)(output_)\n",
    "\n",
    "    output_ = keras.layers.Concatenate()([output_, output_prime])\n",
    "\n",
    "    output_ = keras.layers.Dense(512, activation='relu', kernel_initializer=initializer)(output_)\n",
    "    output_ = keras.layers.Dense(2, activation='softmax')((output_))\n",
    "    return output_\n",
    "\n",
    "\n",
    "def model_flow(input_):\n",
    "    output_ = tf.keras.applications.InceptionV3(input_shape=(224, 224, 3),\n",
    "                               include_top=False,\n",
    "                               weights=None)(input_)\n",
    "\n",
    "    # output_ a fc1000 classification at this point\n",
    "    # Add the self-designed classifier to produce binary result\n",
    "    output_ = classifier_(output_)\n",
    "    return output_\n",
    "\n",
    "\n",
    "def input_output_compile():\n",
    "\n",
    "    # Shape of the training set\n",
    "    input_ = keras.layers.Input(shape=(224, 224, 3))\n",
    "    output_ = model_flow(input_)\n",
    "    clf = keras.Model(inputs=input_, outputs=output_)\n",
    "\n",
    "    clf.compile(optimizer='SGD', loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return clf\n",
    "\n",
    "\n",
    "# Define learning rate scheduler\n",
    "def lr_decay(epoch, lr):\n",
    "    if epoch == 0:\n",
    "\n",
    "        # Do not print evaluation at the beginning of the training\n",
    "\n",
    "        return lr * 1 / (1 + 0.05 * epoch)\n",
    "    if epoch % 5 == 0:\n",
    "        \n",
    "        # Print test accuracy every 5 epochs\n",
    "        print('Test set accuracy: ' +\n",
    "              str(np.round(model.evaluate(x_test, y_test)[1] * 100, 2)) + '%')\n",
    "    return lr * 1 / (1 + 0.05 * epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEyKB7ddMJ1o"
   },
   "source": [
    "### Take a look at the structure of the model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4jXBbeZktgjo",
    "outputId": "9dc33ba1-0f80-4d82-c580-adeeb15bbe4c"
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = input_output_compile()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True,\n",
    "              show_dtype=False,\n",
    "              show_layer_names=False,\n",
    "              show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXfdiMvbMO-q"
   },
   "source": [
    "## Start the training on augmentation generater\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ackaLZ8hs5HG",
    "outputId": "ad55dd8d-8570-40a1-a543-0d59da63143f"
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_decay)\n",
    "hist = model.fit_generator(train_aug.flow(x_train, y_train, batch_size=24),\n",
    "                    epochs=8,\n",
    "                    # steps_per_epoch=X_train.shape[0]//batch_size,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    # validation_steps=X_valid.shape[0]//batch_size\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ctvovc-MWlZ"
   },
   "source": [
    "## Visualize the change of accuracy and loss during the training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kpGImxT4s5AY",
    "outputId": "6fd04bdf-6c12-441a-9581-d237cbbd59c6"
   },
   "outputs": [],
   "source": [
    "acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.figure(figsize = (16, 8))\n",
    "plt.plot(epochs, acc, label='Training acc')\n",
    "plt.plot(epochs, val_acc, label='Validation acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.figure(figsize = (13, 8))\n",
    "plt.plot(epochs, loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8pl90zqMlB9"
   },
   "source": [
    "## Evaluate the result and analyze wrong cases\n",
    "> *    Plot wrong-predicted images\n",
    "> *    Calculate TN, FN, TP, FP\n",
    "> *    Calculate Reall, Precision, Specificity Rates\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pZ1HvP_-Xz_",
    "outputId": "118be517-5f82-4c89-a03f-8dc252944055"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "ID6UhnWm-sbg",
    "outputId": "0e3ce497-3d52-49c8-a923-75d5014d897f"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "correct_ = np.equal(np.argmax(y_pred, 1), y_test)\n",
    "x_wrong = x_test[np.logical_not(correct_)]\n",
    "from re import X\n",
    "# Check if the augmentation works\n",
    "plt.figure(figsize = (20, 15))\n",
    "for i in range(1, len(x_wrong)-1):\n",
    "    plt.subplot(1 + 5 % i, 5, i)\n",
    "    plt.imshow(x_wrong[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWcAaYBmQQ_r",
    "outputId": "9745bbc5-d4a2-4f79-e40e-bbedb7976596"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, 1)\n",
    "TP = np.sum(np.logical_and(y_pred == 0, y_test == 0))\n",
    "FP = np.sum(np.logical_and(y_pred == 0, y_test == 1))\n",
    "TN = np.sum(np.logical_and(y_pred == 1, y_test == 1))\n",
    "FN = np.sum(np.logical_and(y_pred == 1, y_test == 0))\n",
    "print('TP:', TP, '\\nFP:', FP, '\\nTN:', TN, '\\nFN:', FN)\n",
    "print(\"\\nFP: predict as WithoutMask but actually WithMask\",\n",
    "   \"\\nFN: predict as WithMask but actually WithoutMask\")\n",
    "\n",
    "# IMPORTANT when false ALARM is unacceptable\n",
    "precision = str(np.round((TP/(TP+FP))*100, 2)) + \"%\"\n",
    "\n",
    "# IMPORTANT when missing ALARM is unacceptable, \n",
    "# rather have some fake ALARM\n",
    "recall = str(np.round((TP/(TP+FN))*100, 2)) + \"%\"\n",
    "\n",
    "# IMPORTANT when false ALARM is unacceptable\n",
    "specificity = str(np.round((TN/(TN+FP))*100, 2)) + \"%\"\n",
    "\n",
    "print(\"\\nPrecision:\", precision,\"\\nRecall:\", recall,\n",
    "    \"\\nSpecificity:\", specificity)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "CS767.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
